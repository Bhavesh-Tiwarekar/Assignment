{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2fdc04-3241-4849-bfb8-6497d69c2152",
   "metadata": {},
   "source": [
    "##### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5f49cad-cf5a-4903-bef4-04081e65cc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original exam scores: [60 70 75 85 95]\n",
      "Min-Max scaled scores: [[0.        ]\n",
      " [0.28571429]\n",
      " [0.42857143]\n",
      " [0.71428571]\n",
      " [1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNow, all the exam scores are within the range of 0 to 1, making them suitable for input into machine learning algorithms that require features to be on a \\ncommon scale.\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Answer:\n",
    "\n",
    "1. Min-Max scaling is a data preprocessing technique used in machine learning to scale and normalize the values of a feature or dataset to a specific range, \n",
    "typically between 0 and 1. \n",
    "2. This scaling method helps in ensuring that all features have a consistent scale and can be compared on an equal footing, which can be important for many \n",
    "machine learning algorithms that are sensitive to the magnitude of feature values.\n",
    "3. Example:\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset of exam scores\n",
    "exam_scores = np.array([60, 70, 75, 85, 95])\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "min_max = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data using the scaler\n",
    "scaled_scores = min_max.fit_transform(exam_scores.reshape(5,1))\n",
    "\n",
    "print(\"Original exam scores:\", exam_scores)\n",
    "print(\"Min-Max scaled scores:\", scaled_scores)\n",
    "\n",
    "\"\"\"\n",
    "Now, all the exam scores are within the range of 0 to 1, making them suitable for input into machine learning algorithms that require features to be on a \n",
    "common scale.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d780bf94-46a8-48f5-a4b6-03321d8cf82d",
   "metadata": {},
   "source": [
    "##### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad86cd97-9ba3-4e61-beee-b0e09aa1f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vectors:\n",
      "[[3 4]\n",
      " [1 2]\n",
      " [5 5]\n",
      " [2 3]]\n",
      "Normalized vectors (unit vectors):\n",
      "[[0.6        0.8       ]\n",
      " [0.4472136  0.89442719]\n",
      " [0.70710678 0.70710678]\n",
      " [0.5547002  0.83205029]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNow, the vectors have been normalized to unit vectors, each with a magnitude of 1. These unit vectors maintain the relative direction of the original vectors.\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Answer:\n",
    "\n",
    "1. The Unit Vector technique, also known as \"Normalization\" in some contexts, is a feature scaling method used in data preprocessing in machine learning. \n",
    "2. Its purpose is to scale the values of a feature such that they have a magnitude of 1 (unit length) while preserving the direction of the original data. \n",
    "3. Unit Vector scaling doesn't constrain the feature values to a specific range like [0, 1]. Instead, it ensures that all values of the feature are \n",
    "   scaled to have a magnitude of 1 while maintaining their relative proportions.\n",
    "4. Example:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset of 2D vectors\n",
    "vectors = np.array([[3, 4], [1, 2], [5, 5], [2, 3]])\n",
    "\n",
    "# Normalize the vectors\n",
    "normalized_vectors = normalize(vectors)\n",
    "\n",
    "print(\"Original vectors:\")\n",
    "print(vectors)\n",
    "print(\"Normalized vectors (unit vectors):\")\n",
    "print(normalized_vectors)\n",
    "\n",
    "\"\"\"\n",
    "Now, the vectors have been normalized to unit vectors, each with a magnitude of 1. These unit vectors maintain the relative direction of the original vectors.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d7975-727d-4553-a4a4-abbdd6f05a1d",
   "metadata": {},
   "source": [
    "##### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "094b9836-cfe2-4965-a00e-fa5ee7bceff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]\n",
      " [13 14 15]]\n",
      "Reduced data using PCA:\n",
      "[[ 10.39230485   0.        ]\n",
      " [  5.19615242   0.        ]\n",
      " [ -0.           0.        ]\n",
      " [ -5.19615242   0.        ]\n",
      " [-10.39230485   0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe dimensionality of the data has been reduced from 3 to 2, retaining the most important information.\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Answer:\n",
    "\n",
    "1. Principal Component Analysis (PCA) is a dimensionality reduction technique widely used in data analysis and machine learning. \n",
    "2. Its primary goal is to reduce the dimensionality of a dataset while retaining as much relevant information as possible. \n",
    "3. Working of PCA:\n",
    "a. Standardize the data to ensure that all features have a mean of 0 and a standard deviation of 1.\n",
    "b. PCA calculates the covariance matrix of the standardized data to understand the relationships between features.\n",
    "c. PCA then calculates the eigenvectors and eigenvalues of the covariance matrix.\n",
    "d. The eigenvectors are ranked by their corresponding eigenvalues in descending order. You can choose the top-k eigenvectors to retain in the new \n",
    "   feature space, reducing the dimensionality from the original number of features to k.\n",
    "e. The selected eigenvectors form a new basis for the data, and the original data is projected onto this new basis to create the reduced-dimensional dataset.\n",
    "4. Example:\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset with 3 features and 5 data points\n",
    "data = np.array([[1, 2, 3],\n",
    "                 [4, 5, 6],\n",
    "                 [7, 8, 9],\n",
    "                 [10, 11, 12],\n",
    "                 [13, 14, 15]])\n",
    "\n",
    "# Create a PCA object with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data to the new feature space\n",
    "reduced_data = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"Reduced data using PCA:\")\n",
    "print(reduced_data)\n",
    "\n",
    "\"\"\"\n",
    "The dimensionality of the data has been reduced from 3 to 2, retaining the most important information.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c979fe-c1de-44ef-9225-48d0ce6984f2",
   "metadata": {},
   "source": [
    "##### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "##### Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "080aa2fd-6f27-48f5-bb01-a350734a026e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAnswer:\\n\\na. The relationship between PCA and feature extraction can be summarized as follows:\\n1. Dimensionality Reduction: \\n   PCA is primarily used for dimensionality reduction, which is a form of feature extraction. It helps in selecting and creating a smaller number of features \\n   that capture the most relevant information in the original data.\\n2. Preservation of Information: \\n   PCA aims to retain as much variance as possible from the original data when creating the new features (principal components). By doing so, it ensures \\n   that the most important information in the data is preserved.\\n3. Linear Transformation: \\n   PCA performs a linear transformation of the data by projecting it onto a new orthogonal basis formed by the principal components. \\n   These principal components are linear combinations of the original features.\\n\\nb. Example to illustrate how PCA can be used for feature extraction:\\nSuppose you have a dataset of images, and each image is represented by a pixel matrix. Each pixel is considered a feature. If you're dealing with \\nhigh-resolution images, you might have thousands or even millions of features, making the dataset challenging to work with. To use PCA for feature extraction \\nin this context:\\n1. Standardize the pixel values to have a mean of 0 and a standard deviation of 1, ensuring all pixels have the same scale.\\n2. Apply PCA to the standardized pixel data.\\n3. Select a reduced number of principal components based on the amount of variance you want to retain. For example, \\n   you might choose to keep the top 100 principal components out of thousands.\\n4. The selected principal components serve as the new feature vectors for each image, effectively reducing the dimensionality of the data from \\n   thousands of pixels to 100 principal components. These principal components are often much more interpretable and meaningful than individual pixel values.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Answer:\n",
    "\n",
    "a. The relationship between PCA and feature extraction can be summarized as follows:\n",
    "1. Dimensionality Reduction: \n",
    "   PCA is primarily used for dimensionality reduction, which is a form of feature extraction. It helps in selecting and creating a smaller number of features \n",
    "   that capture the most relevant information in the original data.\n",
    "2. Preservation of Information: \n",
    "   PCA aims to retain as much variance as possible from the original data when creating the new features (principal components). By doing so, it ensures \n",
    "   that the most important information in the data is preserved.\n",
    "3. Linear Transformation: \n",
    "   PCA performs a linear transformation of the data by projecting it onto a new orthogonal basis formed by the principal components. \n",
    "   These principal components are linear combinations of the original features.\n",
    "\n",
    "b. Example to illustrate how PCA can be used for feature extraction:\n",
    "Suppose you have a dataset of images, and each image is represented by a pixel matrix. Each pixel is considered a feature. If you're dealing with \n",
    "high-resolution images, you might have thousands or even millions of features, making the dataset challenging to work with. To use PCA for feature extraction \n",
    "in this context:\n",
    "1. Standardize the pixel values to have a mean of 0 and a standard deviation of 1, ensuring all pixels have the same scale.\n",
    "2. Apply PCA to the standardized pixel data.\n",
    "3. Select a reduced number of principal components based on the amount of variance you want to retain. For example, \n",
    "   you might choose to keep the top 100 principal components out of thousands.\n",
    "4. The selected principal components serve as the new feature vectors for each image, effectively reducing the dimensionality of the data from \n",
    "   thousands of pixels to 100 principal components. These principal components are often much more interpretable and meaningful than individual pixel values.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f664dcd-8c1c-4f2e-89de-804e22303eae",
   "metadata": {},
   "source": [
    "##### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "##### contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8126dfb-ea3e-4353-97c6-64e3e1dd3cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer:\\n\\nUsing Min-Max scaling to preprocess the data:\\na. Understand the Data: Here we have a dataset containing price, rating, and delivery time, which may have different ranges and units.\\nb. Calculate the Min and Max Values: For each feature, calculate the minimum and maximum values within the dataset.\\nc. Apply Min-Max Scaling: The formula for Min-Max scaling is:\\n\\nX_new = X - min(X)/ max(X) - min(X)\\n\\nwhere,\\n     X_new: the scaled value of the feature.\\n     X: the original value of the feature.\\n     min(X): the minimum value of the feature in the dataset.\\n     max(X): the maximum value of the feature in the dataset.\\nFor each feature, apply this formula to transform the values into the [0, 1] range.\\nd. Updated Dataset: Replace the original values of the features with their Min-Max scaled values.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Answer:\n",
    "\n",
    "Using Min-Max scaling to preprocess the data:\n",
    "a. Understand the Data: Here we have a dataset containing price, rating, and delivery time, which may have different ranges and units.\n",
    "b. Calculate the Min and Max Values: For each feature, calculate the minimum and maximum values within the dataset.\n",
    "c. Apply Min-Max Scaling: The formula for Min-Max scaling is:\n",
    "\n",
    "X_new = X - min(X)/ max(X) - min(X)\n",
    "\n",
    "where,\n",
    "     X_new: the scaled value of the feature.\n",
    "     X: the original value of the feature.\n",
    "     min(X): the minimum value of the feature in the dataset.\n",
    "     max(X): the maximum value of the feature in the dataset.\n",
    "For each feature, apply this formula to transform the values into the [0, 1] range.\n",
    "d. Updated Dataset: Replace the original values of the features with their Min-Max scaled values.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c7433-f4d3-4bb6-8a6a-88acb68cd1c5",
   "metadata": {},
   "source": [
    "##### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "##### features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83fb05d8-edd4-442f-a202-7b0fe0baa0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer:\\n\\nUsing PCA to reduce the dimensionality of the dataset:\\n1. The first step will be understanding the dataset and performing necessary data preprocessing, including handling missing values, \\n   scaling the data, and ensuring that features are on the same scale.\\n2. The second step is to standardize the data.\\n3. Compute the covariance matrix of the standardized data. \\n4. Calculate the eigenvalues and eigenvectors of the covariance matrix. These eigenvalues represent the amount of variance explained \\n   by each corresponding eigenvector. The eigenvectors are the principal components.\\n5. Sort the eigenvectors in descending order of their associated eigenvalues.\\n6. Decide on the number of principal components to retain. \\n7. Project the original data onto the selected principal components. This creates a reduced-dimensional dataset with a smaller number of features.\\n8. Train the stock price prediction model using the reduced dataset, which contains the selected principal components as features.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Answer:\n",
    "\n",
    "Using PCA to reduce the dimensionality of the dataset:\n",
    "1. The first step will be understanding the dataset and performing necessary data preprocessing, including handling missing values, \n",
    "   scaling the data, and ensuring that features are on the same scale.\n",
    "2. The second step is to standardize the data.\n",
    "3. Compute the covariance matrix of the standardized data. \n",
    "4. Calculate the eigenvalues and eigenvectors of the covariance matrix. These eigenvalues represent the amount of variance explained \n",
    "   by each corresponding eigenvector. The eigenvectors are the principal components.\n",
    "5. Sort the eigenvectors in descending order of their associated eigenvalues.\n",
    "6. Decide on the number of principal components to retain. \n",
    "7. Project the original data onto the selected principal components. This creates a reduced-dimensional dataset with a smaller number of features.\n",
    "8. Train the stock price prediction model using the reduced dataset, which contains the selected principal components as features.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d760eac-132f-4509-88ee-9c0198061203",
   "metadata": {},
   "source": [
    "##### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15ec3024-dbfa-4247-8449-cc3218c43764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n",
      "[[ 1]\n",
      " [ 5]\n",
      " [10]\n",
      " [15]\n",
      " [20]]\n",
      "Min-Max scaled dataset (range -1 to 1):\n",
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "## Program to perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 5, 10, 15, 20]).reshape(5, 1)\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "min_max = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform the data using the scaler\n",
    "scaled_data = min_max.fit_transform(data)\n",
    "\n",
    "print(\"Original dataset:\")\n",
    "print(data)\n",
    "print(\"Min-Max scaled dataset (range -1 to 1):\")\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8da38-17f2-4d6b-9aa5-0dfa7cb27542",
   "metadata": {},
   "source": [
    "##### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "##### Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f5544ab-452e-469b-91fc-a1a08ea63328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance for Each Component:\n",
      "[8.41790118e-01 1.48670310e-01 9.53957255e-03 1.46733664e-33]\n",
      "\n",
      "Cumulative Explained Variance:\n",
      "[0.84179012 0.99046043 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset (height, weight, age, gender, blood pressure)\n",
    "data = np.array([\n",
    "    [170, 65, 30, 1, 120],\n",
    "    [160, 55, 35, 0, 130],\n",
    "    [180, 75, 25, 1, 110],\n",
    "    [175, 70, 28, 0, 125],\n",
    "])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Fit the PCA to the standardized data\n",
    "pca.fit(data_standardized)\n",
    "\n",
    "# Calculate the explained variance for each component\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Create a cumulative explained variance plot\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Determine the number of components to retain (e.g., to explain 95% of the variance)\n",
    "num_components_to_retain = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "print(\"Explained Variance for Each Component:\")\n",
    "print(explained_variance)\n",
    "\n",
    "print(\"\\nCumulative Explained Variance:\")\n",
    "print(cumulative_explained_variance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3d3cbdd-af31-41bd-90d4-91132c7b5f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785984a8-d3b7-44ec-9ef2-e0fc2a9989b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
